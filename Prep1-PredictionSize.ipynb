{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFIED: 24:12 36 hour prediction window\n",
    "\n",
    "In this notebook, we will be modifying the 48 hour prediction window to be a 36 hour prediction window. Using the data from each 24 hour time chunk, we will predict whether or not a patient will get AKI in the next 12 hours. Hopefully, I'll be able to get it such that the doctor can just input whatever settings they want, and this notebook will update accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing sets are separated via patients' id. We use a .npy file found in the Data folder that contains JUST patient IDs, so when we train on the actual dataset, the split is already clear. In this case, Zijian has already split the data and saved it into train.pkl and test.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all patient ids stored in 'HADM_ID' [20001687 20005241 20006999 ... 29994991 29997844 29998115] ; the total number of patients being 4120, \n",
      " which is 147 short from the 4267 supposed patients stated in the article :(\n"
     ]
    }
   ],
   "source": [
    "hadm_id = np.load('C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/all_ids.npy')\n",
    "# randomly spliated 70-30\n",
    "np.random.seed(123) # set seed for reproducibility \n",
    "train_ids = np.random.choice( hadm_id, round(len(hadm_id)*0.7), replace = False )\n",
    "test_ids = np.array(list(set(hadm_id) - set(train_ids)))\n",
    "\n",
    "print(\"all patient ids stored in 'HADM_ID'\", hadm_id, \n",
    "      \"; the total number of patients being 4120, \\n which is 147 short from the 4267 supposed patients stated in the article :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training data: 2884 \n",
      " length of testing data: 1236\n"
     ]
    }
   ],
   "source": [
    "print(\"length of training data:\", len(train_ids), '\\n', \"length of testing data:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason there are 491 hadm_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in folder \"C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily\" is: 491\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# def count_files_in_folder(folder_path):\n",
    "#     folder_path = os.path.join(folder_path, 'C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily')\n",
    "#     num_files = len([name for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))])\n",
    "#     return num_files\n",
    "\n",
    "# folder_path = 'C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily'\n",
    "# num_files = count_files_in_folder(folder_path)\n",
    "# print(f'Number of files in folder \"{folder_path}\" is: {num_files}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revelations\n",
    "\n",
    "Data -> byID: all 2000+ hadm_ids containing each patient's hourly data. \n",
    "\n",
    "Data -> byID_daily: all 2000+ hadm_ids DAILY summaries of their data (basically all of the daily minimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       hadm_id        timedelta  arterial_bp_diastolic  arterial_bp_mean  \\\n",
      "0   20955149.0  0 days 01:00:00                    NaN               NaN   \n",
      "1   20955149.0  0 days 02:00:00                    NaN               NaN   \n",
      "2   20955149.0  0 days 03:00:00                    NaN               NaN   \n",
      "3   20955149.0  0 days 04:00:00              62.470588         77.823529   \n",
      "4   20955149.0  0 days 05:00:00              68.219563         86.328310   \n",
      "5   20955149.0  0 days 06:00:00              63.236763         81.770156   \n",
      "6   20955149.0  0 days 07:00:00              66.000000         85.000000   \n",
      "7   20955149.0  0 days 08:00:00              57.440000         72.000000   \n",
      "8   20955149.0  0 days 09:00:00              69.000000         95.000000   \n",
      "9   20955149.0  0 days 10:00:00              63.000000         84.000000   \n",
      "10  20955149.0  0 days 11:00:00              56.000000         76.000000   \n",
      "11  20955149.0  0 days 12:00:00              53.000000         69.000000   \n",
      "12  20955149.0  0 days 13:00:00              61.000000         80.000000   \n",
      "13  20955149.0  0 days 14:00:00              51.000000         68.000000   \n",
      "14  20955149.0  0 days 15:00:00              55.000000         72.000000   \n",
      "15  20955149.0  0 days 16:00:00              54.000000         70.000000   \n",
      "16  20955149.0  0 days 17:00:00              59.000000         78.000000   \n",
      "17  20955149.0  0 days 18:00:00              58.000000         76.000000   \n",
      "18  20955149.0  0 days 19:00:00              55.000000         73.000000   \n",
      "19  20955149.0  0 days 20:00:00              67.000000         88.000000   \n",
      "20  20955149.0  0 days 21:00:00              62.000000         82.000000   \n",
      "21  20955149.0  0 days 22:00:00              58.000000         76.000000   \n",
      "22  20955149.0  0 days 23:00:00              60.000000         75.000000   \n",
      "23  20955149.0  1 days 00:00:00              75.000000         96.000000   \n",
      "24  20955149.0  1 days 01:00:00              62.000000         83.000000   \n",
      "25  20955149.0  1 days 02:00:00                    NaN               NaN   \n",
      "26  20955149.0  1 days 03:00:00              68.017704         82.008852   \n",
      "27  20955149.0  1 days 04:00:00                    NaN               NaN   \n",
      "28  20955149.0  1 days 05:00:00              77.487033         92.316044   \n",
      "29  20955149.0  1 days 06:00:00              66.000000         82.000000   \n",
      "30  20955149.0  1 days 07:00:00              71.000000         84.000000   \n",
      "31  20955149.0  1 days 08:00:00              76.000000         95.000000   \n",
      "\n",
      "    arterial_bp_systolic  cvp  heart_rate   spo2  pap_diastolic  pap_mean  \\\n",
      "0                    NaN  NaN         NaN    NaN            NaN       NaN   \n",
      "1                    NaN  NaN         NaN    NaN            NaN       NaN   \n",
      "2                    NaN  NaN         NaN    NaN            NaN       NaN   \n",
      "3             101.529412  NaN   82.000000  100.0            NaN       NaN   \n",
      "4             115.911052  NaN   68.954492  100.0            NaN       NaN   \n",
      "5             111.161252  NaN   68.709988  100.0            NaN       NaN   \n",
      "6             114.000000  NaN   70.000000  100.0            NaN       NaN   \n",
      "7              93.560000  NaN   69.320000  100.0            NaN       NaN   \n",
      "8             138.000000  NaN   78.000000  100.0            NaN       NaN   \n",
      "9             122.000000  NaN   73.000000  100.0            NaN       NaN   \n",
      "10            113.000000  NaN   90.000000   97.0            NaN       NaN   \n",
      "11             98.000000  NaN   85.000000   96.0            NaN       NaN   \n",
      "12            116.000000  NaN   87.000000   97.0            NaN       NaN   \n",
      "13             93.000000  NaN   87.000000   98.0            NaN       NaN   \n",
      "14            105.000000  NaN   86.000000   94.0            NaN       NaN   \n",
      "15            102.000000  NaN   82.000000   97.0            NaN       NaN   \n",
      "16            110.000000  NaN   92.000000   96.0            NaN       NaN   \n",
      "17            112.000000  NaN   82.000000   98.0            NaN       NaN   \n",
      "18            111.000000  NaN   78.000000   98.0            NaN       NaN   \n",
      "19            133.000000  NaN   92.000000   96.0            NaN       NaN   \n",
      "20            122.000000  NaN   86.000000   96.0            NaN       NaN   \n",
      "21            113.000000  NaN   80.000000   97.0            NaN       NaN   \n",
      "22            104.000000  NaN   89.000000   96.0            NaN       NaN   \n",
      "23            132.000000  NaN   97.000000   95.0            NaN       NaN   \n",
      "24            131.000000  NaN   84.000000   95.0            NaN       NaN   \n",
      "25                   NaN  NaN   79.000000   97.0            NaN       NaN   \n",
      "26            117.938035  NaN   81.000000   96.0            NaN       NaN   \n",
      "27                   NaN  NaN   89.000000   96.0            NaN       NaN   \n",
      "28            130.401539  NaN   81.000000   97.0            NaN       NaN   \n",
      "29            120.000000  NaN   88.000000   96.0            NaN       NaN   \n",
      "30            113.000000  NaN   80.000000   97.0            NaN       NaN   \n",
      "31            145.000000  NaN   78.000000   98.0            NaN       NaN   \n",
      "\n",
      "    ...  mild_liver_disease  diabetes_without_cc diabetes_with_cc  paraplegia  \\\n",
      "0   ...                 0.0                  0.0              0.0         0.0   \n",
      "1   ...                 0.0                  0.0              0.0         0.0   \n",
      "2   ...                 0.0                  0.0              0.0         0.0   \n",
      "3   ...                 0.0                  0.0              0.0         0.0   \n",
      "4   ...                 0.0                  0.0              0.0         0.0   \n",
      "5   ...                 0.0                  0.0              0.0         0.0   \n",
      "6   ...                 0.0                  0.0              0.0         0.0   \n",
      "7   ...                 0.0                  0.0              0.0         0.0   \n",
      "8   ...                 0.0                  0.0              0.0         0.0   \n",
      "9   ...                 0.0                  0.0              0.0         0.0   \n",
      "10  ...                 0.0                  0.0              0.0         0.0   \n",
      "11  ...                 0.0                  0.0              0.0         0.0   \n",
      "12  ...                 0.0                  0.0              0.0         0.0   \n",
      "13  ...                 0.0                  0.0              0.0         0.0   \n",
      "14  ...                 0.0                  0.0              0.0         0.0   \n",
      "15  ...                 0.0                  0.0              0.0         0.0   \n",
      "16  ...                 0.0                  0.0              0.0         0.0   \n",
      "17  ...                 0.0                  0.0              0.0         0.0   \n",
      "18  ...                 0.0                  0.0              0.0         0.0   \n",
      "19  ...                 0.0                  0.0              0.0         0.0   \n",
      "20  ...                 0.0                  0.0              0.0         0.0   \n",
      "21  ...                 0.0                  0.0              0.0         0.0   \n",
      "22  ...                 0.0                  0.0              0.0         0.0   \n",
      "23  ...                 0.0                  0.0              0.0         0.0   \n",
      "24  ...                 0.0                  0.0              0.0         0.0   \n",
      "25  ...                 0.0                  0.0              0.0         0.0   \n",
      "26  ...                 0.0                  0.0              0.0         0.0   \n",
      "27  ...                 0.0                  0.0              0.0         0.0   \n",
      "28  ...                 0.0                  0.0              0.0         0.0   \n",
      "29  ...                 0.0                  0.0              0.0         0.0   \n",
      "30  ...                 0.0                  0.0              0.0         0.0   \n",
      "31  ...                 0.0                  0.0              0.0         0.0   \n",
      "\n",
      "    renal_disease  malignant_cancer  severe_liver_disease  \\\n",
      "0             0.0               0.0                   0.0   \n",
      "1             0.0               0.0                   0.0   \n",
      "2             0.0               0.0                   0.0   \n",
      "3             0.0               0.0                   0.0   \n",
      "4             0.0               0.0                   0.0   \n",
      "5             0.0               0.0                   0.0   \n",
      "6             0.0               0.0                   0.0   \n",
      "7             0.0               0.0                   0.0   \n",
      "8             0.0               0.0                   0.0   \n",
      "9             0.0               0.0                   0.0   \n",
      "10            0.0               0.0                   0.0   \n",
      "11            0.0               0.0                   0.0   \n",
      "12            0.0               0.0                   0.0   \n",
      "13            0.0               0.0                   0.0   \n",
      "14            0.0               0.0                   0.0   \n",
      "15            0.0               0.0                   0.0   \n",
      "16            0.0               0.0                   0.0   \n",
      "17            0.0               0.0                   0.0   \n",
      "18            0.0               0.0                   0.0   \n",
      "19            0.0               0.0                   0.0   \n",
      "20            0.0               0.0                   0.0   \n",
      "21            0.0               0.0                   0.0   \n",
      "22            0.0               0.0                   0.0   \n",
      "23            0.0               0.0                   0.0   \n",
      "24            0.0               0.0                   0.0   \n",
      "25            0.0               0.0                   0.0   \n",
      "26            0.0               0.0                   0.0   \n",
      "27            0.0               0.0                   0.0   \n",
      "28            0.0               0.0                   0.0   \n",
      "29            0.0               0.0                   0.0   \n",
      "30            0.0               0.0                   0.0   \n",
      "31            0.0               0.0                   0.0   \n",
      "\n",
      "    metastatic_solid_tumor  aids  AKI_any  \n",
      "0                      0.0   0.0      0.0  \n",
      "1                      0.0   0.0      0.0  \n",
      "2                      0.0   0.0      0.0  \n",
      "3                      0.0   0.0      0.0  \n",
      "4                      0.0   0.0      0.0  \n",
      "5                      0.0   0.0      0.0  \n",
      "6                      0.0   0.0      0.0  \n",
      "7                      0.0   0.0      0.0  \n",
      "8                      0.0   0.0      0.0  \n",
      "9                      0.0   0.0      0.0  \n",
      "10                     0.0   0.0      0.0  \n",
      "11                     0.0   0.0      0.0  \n",
      "12                     0.0   0.0      0.0  \n",
      "13                     0.0   0.0      0.0  \n",
      "14                     0.0   0.0      0.0  \n",
      "15                     0.0   0.0      0.0  \n",
      "16                     0.0   0.0      0.0  \n",
      "17                     0.0   0.0      0.0  \n",
      "18                     0.0   0.0      0.0  \n",
      "19                     0.0   0.0      0.0  \n",
      "20                     0.0   0.0      0.0  \n",
      "21                     0.0   0.0      0.0  \n",
      "22                     0.0   0.0      0.0  \n",
      "23                     0.0   0.0      0.0  \n",
      "24                     0.0   0.0      0.0  \n",
      "25                     0.0   0.0      0.0  \n",
      "26                     0.0   0.0      0.0  \n",
      "27                     0.0   0.0      0.0  \n",
      "28                     0.0   0.0      0.0  \n",
      "29                     0.0   0.0      0.0  \n",
      "30                     0.0   0.0      0.0  \n",
      "31                     0.0   0.0      0.0  \n",
      "\n",
      "[32 rows x 577 columns]\n"
     ]
    }
   ],
   "source": [
    "# prep test code for actual code below\n",
    "temp = pd.read_pickle('C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID/20955149.pkl')\n",
    "print(temp)\n",
    "\n",
    "row_count=0\n",
    "\n",
    "temp_df= pd.DateFrame()\n",
    "\n",
    "for id in train_ids:\n",
    "    for row in id:\n",
    "        row_count+=1\n",
    "\n",
    "    rows_to_process= temp[0:12]\n",
    "    min_values = np.amin(rows_to_process, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>arterial_bp_diastolic_min</th>\n",
       "      <th>arterial_bp_mean_min</th>\n",
       "      <th>arterial_bp_systolic_min</th>\n",
       "      <th>cvp_min</th>\n",
       "      <th>heart_rate_min</th>\n",
       "      <th>spo2_min</th>\n",
       "      <th>pap_diastolic_min</th>\n",
       "      <th>pap_mean_min</th>\n",
       "      <th>pap_systolic_min</th>\n",
       "      <th>...</th>\n",
       "      <th>mild_liver_disease</th>\n",
       "      <th>diabetes_without_cc</th>\n",
       "      <th>diabetes_with_cc</th>\n",
       "      <th>paraplegia</th>\n",
       "      <th>renal_disease</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>severe_liver_disease</th>\n",
       "      <th>metastatic_solid_tumor</th>\n",
       "      <th>aids</th>\n",
       "      <th>AKI_any</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20955149.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.709988</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20955149.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id  arterial_bp_diastolic_min  arterial_bp_mean_min  \\\n",
       "0  20955149.0                       51.0                  68.0   \n",
       "1  20955149.0                       62.0                  82.0   \n",
       "\n",
       "   arterial_bp_systolic_min  cvp_min  heart_rate_min  spo2_min  \\\n",
       "0                      93.0      NaN       68.709988      94.0   \n",
       "1                     113.0      NaN       78.000000      95.0   \n",
       "\n",
       "   pap_diastolic_min  pap_mean_min  pap_systolic_min  ...  mild_liver_disease  \\\n",
       "0                NaN           NaN               NaN  ...                 0.0   \n",
       "1                NaN           NaN               NaN  ...                 0.0   \n",
       "\n",
       "   diabetes_without_cc  diabetes_with_cc  paraplegia  renal_disease  \\\n",
       "0                  0.0               0.0         0.0            0.0   \n",
       "1                  0.0               0.0         0.0            0.0   \n",
       "\n",
       "   malignant_cancer  severe_liver_disease  metastatic_solid_tumor  aids  \\\n",
       "0               0.0                   0.0                     0.0   0.0   \n",
       "1               0.0                   0.0                     0.0   0.0   \n",
       "\n",
       "   AKI_any  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "\n",
       "[2 rows x 1121 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prep test code for actual code below\n",
    "temp = pd.read_pickle('C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily/20955149.pkl')\n",
    "temp\n",
    "\n",
    "#this is the daily patient aki result i guess??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering 'daily_data' is relatively robust (less affected by inaccurate time indicators)  \n",
    "We re-arrange the data, using parameters in today, predicting AKI indicator tomorrow (24 hrs ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2884 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily25630745.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_ids))):\n\u001b[0;32m      6\u001b[0m     tem_id \u001b[38;5;241m=\u001b[39m train_ids[idx]\n\u001b[1;32m----> 7\u001b[0m     tem_daily \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtem_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tem_daily) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     10\u001b[0m         tem_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(  [tem_daily\u001b[38;5;241m.\u001b[39miloc[ \u001b[38;5;241m0\u001b[39m:(\u001b[38;5;28mlen\u001b[39m(tem_daily)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)],  \n\u001b[0;32m     11\u001b[0m                      pd\u001b[38;5;241m.\u001b[39mDataFrame(tem_daily[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAKI_any\u001b[39m\u001b[38;5;124m'\u001b[39m ]\u001b[38;5;241m.\u001b[39miloc[ \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mlen\u001b[39m(tem_daily) ]\u001b[38;5;241m.\u001b[39mvalues) ]\n\u001b[0;32m     12\u001b[0m                     , axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     13\u001b[0m                     )\u001b[38;5;241m.\u001b[39mdrop( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAKI_any\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\faith\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\faith\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily25630745.pkl'"
     ]
    }
   ],
   "source": [
    "# prepare training set\n",
    "training_appended = []\n",
    "training = pd.DataFrame()\n",
    "\n",
    "for idx in tqdm(range(len(train_ids))):\n",
    "    tem_id = train_ids[idx]\n",
    "    tem_daily = pd.read_pickle( 'C:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily' + str(tem_id) + '.pkl')\n",
    "    \n",
    "    if len(tem_daily) > 2: #for all hadm_ids that represent patients that stayed in  the icu for more than two days\n",
    "        tem_train = pd.concat(  [tem_daily.iloc[ 0:(len(tem_daily)-2)],  #FROM DAILYIDS: take info for all rows (days) except for the SECOND TO last day and last day\n",
    "                     pd.DataFrame(tem_daily[ 'AKI_any' ].iloc[ 2:len(tem_daily) ].values) ] #extract the AKI_any column values from the second to last day to end\n",
    "                    , axis = 1,\n",
    "                    ).drop( 'AKI_any', axis = 1) #concatenate the second line to the first line such that it is displaced (AKI_any thus becomes AKI_in_24) and the last day is missing a value\n",
    "        \n",
    "        training_appended.append(tem_train) #each modified dataframe stored in tem_train and added to training_appended list\n",
    "        \n",
    "training = pd.concat(training_appended) #initially empty training df gets all modified dfs added to it.\n",
    "training.rename( columns = {0:'AKI_in_48'}, inplace = True)\n",
    "training.to_pickle('train_set_48_faith.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used a faster way by list to store everything\n",
    "\n",
    "# # prepare training set\n",
    "# training = pd.DataFrame()\n",
    "# for idx in tqdm(range(len(train_ids))):\n",
    "#     tem_id = train_ids[idx]\n",
    "#     tem_daily = pd.read_pickle( '../Data/byID_daily/' + str(tem_id) + '.pkl')\n",
    "    \n",
    "#     if len(tem_daily) > 1:\n",
    "#         tem_train = pd.concat(  [tem_daily.iloc[ 0:(len(tem_daily)-1)],  \n",
    "#                      pd.DataFrame(tem_daily[ 'AKI_any' ].iloc[ 1:len(tem_daily) ].values) ]\n",
    "#                     , axis = 1,\n",
    "#                     ).drop( 'AKI_any', axis = 1)\n",
    "        \n",
    "#         training = pd.concat( [training, tem_train], ignore_index = True )\n",
    "        \n",
    "# training.rename( columns = {0:'AKI_in_24'}, inplace = True)\n",
    "# training.to_pickle('./train_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training if loss connection\n",
    "training = pd.read_pickle('./train_set.pkl')\n",
    "\n",
    "testing = pd.DataFrame( columns = np.concatenate( [ ['time'], training.columns.values]))\n",
    "inform = pd.DataFrame( columns = ['hadm_id', 'if_AKI_inICU','first_AKI_detected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing set will be the rolling window, using ( max(0, t-24), t) to predict (t+1, t+24)  \n",
    "Record additional information such as  \n",
    "'if the patients develop AKI during ICU'  \n",
    "'when is the first time that the patient is found AKI'  \n",
    "for the future report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING INFORM and TESTING datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [27:03<00:00,  1.31s/it] \n"
     ]
    }
   ],
   "source": [
    "testing_appended = []\n",
    "inform_appended = []\n",
    "\n",
    "# prepare testing\n",
    "for idx in tqdm(range(len(test_ids))):\n",
    "    tem_id = test_ids[idx]\n",
    "    tem_patient = pd.read_pickle( '../Data/byID/' + str(tem_id) + '.pkl')\n",
    "    time_stamps = tem_patient['timedelta']\n",
    "    \n",
    "    tem_testing = pd.DataFrame( columns = np.concatenate( [ ['time'], training.columns.values]))\n",
    "    for time_id in range(len(time_stamps)):\n",
    "        if time_id != (len(time_stamps) - 1):\n",
    "            tem = pd.DataFrame( data = np.concatenate( (np.array([time_id, tem_id]),\n",
    "                    tem_patient.loc[ tem_patient['timedelta'].isin( time_stamps[ max(0,(time_id-23)) : (time_id+1)].values ) ].drop( ['hadm_id','timedelta','Heart Rhythm','admission_category'], axis = 1 ).min(axis = 0).values[0:547],\n",
    "                    tem_patient.loc[ tem_patient['timedelta'].isin( time_stamps[ max(0,(time_id-23)) : (time_id+1)].values ) ].drop( ['hadm_id','timedelta','Heart Rhythm','admission_category'], axis = 1 ).max(axis = 0).values[0:572],\n",
    "                    np.array( [tem_patient.loc[ tem_patient['timedelta'].isin( time_stamps[ (time_id+1) : min(len(time_stamps), time_id+25 )].values ), 'AKI_any' ].max()])\n",
    "                    )).reshape( [1,-1] ),\n",
    "                    columns = testing.columns.values)\n",
    "            \n",
    "            testing_appended.append(tem)\n",
    "\n",
    "    if tem_patient['AKI_any'].max() == 0:\n",
    "        new_inform = pd.DataFrame( data = np.array([tem_id, tem_patient['AKI_any'].max(), float('NaN')]).reshape([1,-1]),\n",
    "                    columns = ['hadm_id', 'if_AKI_inICU','first_AKI_detected'])\n",
    "    else:\n",
    "        new_inform = pd.DataFrame( data = np.array([tem_id, tem_patient['AKI_any'].max(), tem_patient.loc[ tem_patient['AKI_any'] == 1, 'timedelta'].index[0]]).reshape([1,-1]),\n",
    "                    columns = ['hadm_id', 'if_AKI_inICU','first_AKI_detected'])\n",
    "        \n",
    "    inform_appended.append(new_inform)\n",
    "    \n",
    "\n",
    "#testing.to_pickle('./testing/test_set.pkl')\n",
    "#inform.to_pickle('./testing/inform.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.concat(testing_appended)\n",
    "inform = pd.concat(inform_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.loc[testing['gender'] == 'M', 'gender'] = 0\n",
    "testing.loc[testing['gender'] == 'F', 'gender'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = testing.astype( 'float' )\n",
    "testing.to_pickle('./test_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inform.to_pickle('./inform.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
