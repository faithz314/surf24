{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing sets are separated via patients' id. We use a .npy file found in the Data folder that contains JUST patient IDs, so when we train on the actual dataset, the split is already clear. In this case, Zijian has already split the data and saved it into train.pkl and test.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all patient ids [20001687 20005241 20006999 ... 29994991 29997844 29998115] ; the total number of patients being 4120, \n",
      " which is 147 short from the 4267 supposed patients stated in the article :(\n"
     ]
    }
   ],
   "source": [
    "hadm_id = np.load('C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/all_ids.npy')\n",
    "# randomly spliated 70-30\n",
    "np.random.seed(123) # set seed for reproducibility \n",
    "train_ids = np.random.choice( hadm_id, round(len(hadm_id)*0.7), replace = False )\n",
    "test_ids = np.array(list(set(hadm_id) - set(train_ids)))\n",
    "\n",
    "print(\"all patient ids\", hadm_id, \n",
    "      \"; the total number of patients being 4120, \\n which is 147 short from the 4267 supposed patients stated in the article :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training data: 2884 \n",
      " length of testing data: 1236\n"
     ]
    }
   ],
   "source": [
    "print(\"length of training data:\", len(train_ids), '\\n', \"length of testing data:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason there are 491 hadm_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in folder \"C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily\" is: 491\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# def count_files_in_folder(folder_path):\n",
    "#     folder_path = os.path.join(folder_path, 'C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily')\n",
    "#     num_files = len([name for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))])\n",
    "#     return num_files\n",
    "\n",
    "# folder_path = 'C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily'\n",
    "# num_files = count_files_in_folder(folder_path)\n",
    "# print(f'Number of files in folder \"{folder_path}\" is: {num_files}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revelations\n",
    "\n",
    "Data -> byID: all 2000+ hadm_ids containing each patient's hourly data. \n",
    "\n",
    "Data -> byID_daily: all 2000+ hadm_ids DAILY summaries of their data (basically all of the daily minimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.0\n"
     ]
    }
   ],
   "source": [
    "# prep test code for actual code below\n",
    "temp = pd.read_pickle('C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID/20955149.pkl')\n",
    "print(temp['arterial_bp_mean'].iloc[0:24].min())\n",
    "\n",
    "#this is the daily patient aki result i guess??"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"AKI Any\" is replaced with \"AKI in 24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
=======
   "cell_type": "code",
   "execution_count": 7,
>>>>>>> 2d01cc5f9de089da081e7e5228cdf9cfb413e826
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>arterial_bp_diastolic_min</th>\n",
       "      <th>arterial_bp_mean_min</th>\n",
       "      <th>arterial_bp_systolic_min</th>\n",
       "      <th>cvp_min</th>\n",
       "      <th>heart_rate_min</th>\n",
       "      <th>spo2_min</th>\n",
       "      <th>pap_diastolic_min</th>\n",
       "      <th>pap_mean_min</th>\n",
       "      <th>pap_systolic_min</th>\n",
       "      <th>...</th>\n",
       "      <th>mild_liver_disease</th>\n",
       "      <th>diabetes_without_cc</th>\n",
       "      <th>diabetes_with_cc</th>\n",
       "      <th>paraplegia</th>\n",
       "      <th>renal_disease</th>\n",
       "      <th>malignant_cancer</th>\n",
       "      <th>severe_liver_disease</th>\n",
       "      <th>metastatic_solid_tumor</th>\n",
       "      <th>aids</th>\n",
       "      <th>AKI_any</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20955149.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.709988</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20955149.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hadm_id  arterial_bp_diastolic_min  arterial_bp_mean_min  \\\n",
       "0  20955149.0                       51.0                  68.0   \n",
       "1  20955149.0                       62.0                  82.0   \n",
       "\n",
       "   arterial_bp_systolic_min  cvp_min  heart_rate_min  spo2_min  \\\n",
       "0                      93.0      NaN       68.709988      94.0   \n",
       "1                     113.0      NaN       78.000000      95.0   \n",
       "\n",
       "   pap_diastolic_min  pap_mean_min  pap_systolic_min  ...  mild_liver_disease  \\\n",
       "0                NaN           NaN               NaN  ...                 0.0   \n",
       "1                NaN           NaN               NaN  ...                 0.0   \n",
       "\n",
       "   diabetes_without_cc  diabetes_with_cc  paraplegia  renal_disease  \\\n",
       "0                  0.0               0.0         0.0            0.0   \n",
       "1                  0.0               0.0         0.0            0.0   \n",
       "\n",
       "   malignant_cancer  severe_liver_disease  metastatic_solid_tumor  aids  \\\n",
       "0               0.0                   0.0                     0.0   0.0   \n",
       "1               0.0                   0.0                     0.0   0.0   \n",
       "\n",
       "   AKI_any  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "\n",
       "[2 rows x 1121 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 7,
>>>>>>> 2d01cc5f9de089da081e7e5228cdf9cfb413e826
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prep test code for actual code below\n",
    "temp = pd.read_pickle('C:/Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily/20955149.pkl')\n",
    "temp\n",
    "\n",
    "#this is the daily patient aki result i guess??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering 'daily_data' is relatively robust (less affected by inaccurate time indicators)  \n",
    "We re-arrange the data, using parameters in today, predicting AKI indicator tomorrow (24 hrs ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2884 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily25630745.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_ids))):\n\u001b[0;32m      6\u001b[0m     tem_id \u001b[38;5;241m=\u001b[39m train_ids[idx]\n\u001b[1;32m----> 7\u001b[0m     tem_daily \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtem_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tem_daily) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     10\u001b[0m         tem_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(  [tem_daily\u001b[38;5;241m.\u001b[39miloc[ \u001b[38;5;241m0\u001b[39m:(\u001b[38;5;28mlen\u001b[39m(tem_daily)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)],  \n\u001b[0;32m     11\u001b[0m                      pd\u001b[38;5;241m.\u001b[39mDataFrame(tem_daily[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAKI_any\u001b[39m\u001b[38;5;124m'\u001b[39m ]\u001b[38;5;241m.\u001b[39miloc[ \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mlen\u001b[39m(tem_daily) ]\u001b[38;5;241m.\u001b[39mvalues) ]\n\u001b[0;32m     12\u001b[0m                     , axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     13\u001b[0m                     )\u001b[38;5;241m.\u001b[39mdrop( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAKI_any\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\faith\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\faith\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily25630745.pkl'"
     ]
    }
   ],
   "source": [
    "# prepare training set\n",
    "training_appended = []\n",
    "training = pd.DataFrame()\n",
    "\n",
    "for idx in tqdm(range(len(train_ids))):\n",
    "    tem_id = train_ids[idx]\n",
    "    tem_daily = pd.read_pickle( 'C:Users/faith/OneDrive/Documents/FaithZhang/SURF/RealTimeDetection/Data/byID_daily' + str(tem_id) + '.pkl')\n",
    "    \n",
    "    if len(tem_daily) > 1: #for all hadm_ids that represent patients that stayed in  the icu for more than one day\n",
    "        tem_train = pd.concat(  [tem_daily.iloc[ 0:(len(tem_daily)-1)],  #FROM DAILYIDS: take info for all rows (days) except for the last day\n",
    "                     pd.DataFrame(tem_daily[ 'AKI_any' ].iloc[ 1:len(tem_daily) ].values) ] #extract the AKI_any column values from the second row (day) to the end row (day)\n",
    "                    , axis = 1,\n",
    "                    ).drop( 'AKI_any', axis = 1) #concatenate the second line to the first line such that it is displaced (AKI_any thus becomes AKI_in_24) and the last day is missing a value\n",
    "        \n",
    "        training_appended.append(tem_train) #each modified dataframe stored in tem_train and added to training_appended list\n",
    "        \n",
    "training = pd.concat(training_appended) #initially empty training df gets all modified dfs added to it.\n",
    "training.rename( columns = {0:'AKI_in_24'}, inplace = True)\n",
    "training.to_pickle('./train_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used a faster way by list to store everything\n",
    "\n",
    "# # prepare training set\n",
    "# training = pd.DataFrame()\n",
    "# for idx in tqdm(range(len(train_ids))):\n",
    "#     tem_id = train_ids[idx]\n",
    "#     tem_daily = pd.read_pickle( '../Data/byID_daily/' + str(tem_id) + '.pkl')\n",
    "    \n",
    "#     if len(tem_daily) > 1:\n",
    "#         tem_train = pd.concat(  [tem_daily.iloc[ 0:(len(tem_daily)-1)],  \n",
    "#                      pd.DataFrame(tem_daily[ 'AKI_any' ].iloc[ 1:len(tem_daily) ].values) ]\n",
    "#                     , axis = 1,\n",
    "#                     ).drop( 'AKI_any', axis = 1)\n",
    "        \n",
    "#         training = pd.concat( [training, tem_train], ignore_index = True )\n",
    "        \n",
    "# training.rename( columns = {0:'AKI_in_24'}, inplace = True)\n",
    "# training.to_pickle('./train_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training if loss connection\n",
    "training = pd.read_pickle('./train_set.pkl')\n",
    "\n",
    "testing = pd.DataFrame( columns = np.concatenate( [ ['time'], training.columns.values]))\n",
    "inform = pd.DataFrame( columns = ['hadm_id', 'if_AKI_inICU','first_AKI_detected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing set will be the rolling window, using ( max(0, t-24), t) to predict (t+1, t+24)  \n",
    "Record additional information such as  \n",
    "'if the patients develop AKI during ICU'  \n",
    "'when is the first time that the patient is found AKI'  \n",
    "for the future report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING INFORM and TESTING datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1236/1236 [27:03<00:00,  1.31s/it] \n"
     ]
    }
   ],
   "source": [
    "testing_appended = []\n",
    "inform_appended = []\n",
    "\n",
    "# prepare testing\n",
    "for idx in tqdm(range(len(test_ids))):\n",
    "    tem_id = test_ids[idx]\n",
    "    tem_patient = pd.read_pickle( '../Data/byID/' + str(tem_id) + '.pkl')\n",
    "    time_stamps = tem_patient['timedelta']\n",
    "    \n",
    "    tem_testing = pd.DataFrame( columns = np.concatenate( [ ['time'], training.columns.values]))\n",
    "    for time_id in range(len(time_stamps)):\n",
    "        if time_id != (len(time_stamps) - 1):\n",
    "            tem = pd.DataFrame( data = np.concatenate( (np.array([time_id, tem_id]),\n",
    "                    tem_patient.loc[ tem_patient['timedelta'].isin( time_stamps[ max(0,(time_id-23)) : (time_id+1)].values ) ].drop( ['hadm_id','timedelta','Heart Rhythm','admission_category'], axis = 1 ).min(axis = 0).values[0:547],\n",
    "                    tem_patient.loc[ tem_patient['timedelta'].isin( time_stamps[ max(0,(time_id-23)) : (time_id+1)].values ) ].drop( ['hadm_id','timedelta','Heart Rhythm','admission_category'], axis = 1 ).max(axis = 0).values[0:572],\n",
    "                    np.array( [tem_patient.loc[ tem_patient['timedelta'].isin( time_stamps[ (time_id+1) : min(len(time_stamps), time_id+25 )].values ), 'AKI_any' ].max()])\n",
    "                    )).reshape( [1,-1] ),\n",
    "                    columns = testing.columns.values)\n",
    "            \n",
    "            testing_appended.append(tem)\n",
    "\n",
    "    if tem_patient['AKI_any'].max() == 0:\n",
    "        new_inform = pd.DataFrame( data = np.array([tem_id, tem_patient['AKI_any'].max(), float('NaN')]).reshape([1,-1]),\n",
    "                    columns = ['hadm_id', 'if_AKI_inICU','first_AKI_detected'])\n",
    "    else:\n",
    "        new_inform = pd.DataFrame( data = np.array([tem_id, tem_patient['AKI_any'].max(), tem_patient.loc[ tem_patient['AKI_any'] == 1, 'timedelta'].index[0]]).reshape([1,-1]),\n",
    "                    columns = ['hadm_id', 'if_AKI_inICU','first_AKI_detected'])\n",
    "        \n",
    "    inform_appended.append(new_inform)\n",
    "    \n",
    "\n",
    "#testing.to_pickle('./testing/test_set.pkl')\n",
    "#inform.to_pickle('./testing/inform.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.concat(testing_appended)\n",
    "inform = pd.concat(inform_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.loc[testing['gender'] == 'M', 'gender'] = 0\n",
    "testing.loc[testing['gender'] == 'F', 'gender'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = testing.astype( 'float' )\n",
    "testing.to_pickle('./test_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inform.to_pickle('./inform.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
